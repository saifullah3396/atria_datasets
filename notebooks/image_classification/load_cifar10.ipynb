{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Dataset Handling with Atria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Auto-reloading Modules\n",
    "We enable auto-reloading of modules so that any changes in imported libraries are automatically reflected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies\n",
    "Here, we modify the system path to include the project's root directory and import necessary modules for dataset handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the CIFAR-10 Dataset\n",
    "We load the CIFAR-10 dataset using the `CIFAR10.load` method, specifying the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-21 20:06:23][atria_datasets.core.dataset.atria_dataset][INFO] Loading dataset cifar10 from registry.\n",
      "[2025-07-21 20:06:23][atria_datasets.core.dataset.atria_dataset][INFO] Setting up data directory: /mnt/hephaistos/.atria/datasets/cifar10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Cifar10' object has no attribute '_storage_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01matria_datasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AtriaImageDataset, FileStorageType\n\u001b[32m      5\u001b[39m package_path = _get_package_base_path(\u001b[33m\"\u001b[39m\u001b[33matria\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m dataset = \u001b[43mAtriaImageDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_from_registry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcifar10\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43matria_datasets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuild_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_train_samples\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_test_samples\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_validation_samples\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m dataset.train.dataframe()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:34\u001b[39m, in \u001b[36mload_from_registry\u001b[39m\u001b[34m(cls, name, split, data_dir, provider, preprocess_transform, shard_storage_type, access_token, overwrite_existing_cached, overwrite_existing_shards, allowed_keys, build_kwargs, sharded_storage_kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hephaistos/projects/atria_agent/atria/atria_datasets/src/atria_datasets/core/dataset/atria_dataset.py:367\u001b[39m, in \u001b[36mAtriaDataset.build_split\u001b[39m\u001b[34m(self, split, data_dir, runtime_transforms, preprocess_transform, access_token, overwrite_existing_cached, overwrite_existing_shards, allowed_keys, enable_cached_splits, **sharded_storage_kwargs)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# Prepare splits based on caching preference\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m enable_cached_splits:\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_cached_splits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverwrite_existing\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_existing_cached\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallowed_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m._prepare_uncached_splits(\n\u001b[32m    375\u001b[39m         split=split, allowed_keys=allowed_keys, access_token=access_token\n\u001b[32m    376\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hephaistos/projects/atria_agent/atria/atria_datasets/src/atria_datasets/core/dataset/atria_dataset.py:706\u001b[39m, in \u001b[36mAtriaDataset._prepare_cached_splits\u001b[39m\u001b[34m(self, split, access_token, write_batch_size, overwrite_existing, allowed_keys)\u001b[39m\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01matria_datasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstorage\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeltalake_storage_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    700\u001b[39m     DeltalakeStorageManager,\n\u001b[32m    701\u001b[39m )\n\u001b[32m    703\u001b[39m splits = [split] \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(DatasetSplitType)\n\u001b[32m    705\u001b[39m storage_manager = DeltalakeStorageManager(\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m     storage_dir=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_storage_dir\u001b[49m), write_batch_size=write_batch_size\n\u001b[32m    707\u001b[39m )\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m current_split \u001b[38;5;129;01min\u001b[39;00m splits:\n\u001b[32m    710\u001b[39m     split_exists = storage_manager.split_exists(split=current_split)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Cifar10' object has no attribute '_storage_dir'"
     ]
    }
   ],
   "source": [
    "from atria_core.utilities.imports import _get_package_base_path\n",
    "\n",
    "from atria_datasets import AtriaImageDataset, FileStorageType\n",
    "\n",
    "package_path = _get_package_base_path(\"atria\")\n",
    "dataset = AtriaImageDataset.load_from_registry(\n",
    "    name=\"cifar10\",\n",
    "    provider=\"atria_datasets\",\n",
    "    build_kwargs ={\n",
    "        \"max_train_samples\": 1000,\n",
    "        \"max_test_samples\": 1000,\n",
    "        \"max_validation_samples\": 1000,\n",
    "    }\n",
    ")\n",
    "dataset.train.dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating batched instances from a list of samples\n",
    "We create a list of samples and then call batched on the list which is the class method of the specific instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-11 12:52:15][atria_core.types.base._mixins._batchable][WARNING] Found inputs with partial None values in field ser. Ignoring field for batch.\n",
      "[2025-07-11 12:52:15][atria_core.types.base._mixins._batchable][WARNING] Found inputs with partial None values in field ocr. Ignoring field for batch.\n",
      "[2025-07-11 12:52:15][atria_core.types.base._mixins._batchable][WARNING] Found inputs with partial None values in field qa. Ignoring field for batch.\n",
      "[2025-07-11 12:52:15][atria_core.types.base._mixins._batchable][WARNING] Found inputs with partial None values in field vqa. Ignoring field for batch.\n",
      "[2025-07-11 12:52:15][atria_core.types.base._mixins._batchable][WARNING] Found inputs with partial None values in field layout. Ignoring field for batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# Make a list of instances\n",
    "instances = [\n",
    "    dataset.train[i].to_tensor() for i in range(2)\n",
    "]\n",
    "\n",
    "# Batch the instances\n",
    "batched = instances[0].batched(instances)\n",
    "\n",
    "# Display the batched instances\n",
    "print(batched.image.content[0] - dataset.train[0].to_tensor().image.content)\n",
    "print(batched.image.content[1] - dataset.train[1].to_tensor().image.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset handling with File Storage\n",
    "Load the dataset with a file storage manager that first caches the data into disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FileStorageManager.__init__() got an unexpected keyword argument 'storage_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01matria\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstorage\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutilities\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileStorageType\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Creat a file storage manager\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m file_storage_manager = \u001b[43mFileStorageManager\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/tmp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreaming_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFileStorageType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMSGPACK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# save up to 100 samples\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: FileStorageManager.__init__() got an unexpected keyword argument 'storage_dir'"
     ]
    }
   ],
   "source": [
    "from atria.data.storage.file_storage_manager import FileStorageManager\n",
    "from atria.data.storage.utilities import FileStorageType\n",
    "\n",
    "# Creat a file storage manager\n",
    "file_storage_manager = FileStorageManager(\n",
    "    storage_dir=\"/tmp\", streaming_mode=False, storage_type=FileStorageType.MSGPACK, \n",
    "    max_samples=100, # save up to 100 samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Cifar10' has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01matria_examples\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcifar10\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cifar10\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# load the dataset with the file storage manager\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m cifar10 = \u001b[43mCifar10\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m(\n\u001b[32m      6\u001b[39m     split=DatasetSplit.train,\n\u001b[32m      7\u001b[39m     storage_manager=file_storage_manager,\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: type object 'Cifar10' has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "from atria_core.types import DatasetSplitType\n",
    "from atria_examples.datasets.cifar10 import Cifar10\n",
    "\n",
    "# load the dataset with the file storage manager\n",
    "cifar10 = Cifar10.load(\n",
    "    split=DatasetSplitType.train,\n",
    "    storage_manager=file_storage_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageInstance(\n",
       "    index=0,\n",
       "    id=UUID('629095eb-cad3-4e9d-8b27-6e989ab27bba'),\n",
       "    image=Image(\n",
       "        file_path=None,\n",
       "        content=<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x7760B0786900>,\n",
       "        source_size=None,\n",
       "        shape=(3, 32, 32),\n",
       "        dtype=None\n",
       "    ),\n",
       "    label=Label(value=6, name='frog')\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract a sample instance from the dataset\n",
    "cifar10[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atria-datasets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
